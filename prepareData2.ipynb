{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports packages and assign variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import re\n",
    "import csv\n",
    "file_paths = [\n",
    "    'testData/21.10_A+NA+AZA_30s_600ms_3/',\n",
    "    'testData/22.130_A+NA+AZA_30s_600ms_3/',\n",
    "    'testData/23.60_A+NA+AZA_30s_200ms_3/',\n",
    "    'testData/24.60_A+NA+AZA_30s_1100ms_3/',\n",
    "    'testData/25.60_A+NA+AZA_5s_600ms_3/',\n",
    "    'testData/26.60_A+NA+AZA_50s_600ms_3/',\n",
    "    'testData/27.60_A+NA+AZA_30s_600ms_2/',\n",
    "    'testData/28.60_A+NA+AZA_30s_600ms_8/',\n",
    "]\n",
    "\n",
    "labels = [ _[12:-1] if _ [11] == '.' else _[11:-1] for _ in file_paths ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert .sql to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(column_name):\n",
    "    column_name = column_name.replace('\"', '')\n",
    "    column_name  = re.sub(r'(?<!^)(?=[A-Z])', '_', column_name).lower()\n",
    "    if column_name == 'requestid':\n",
    "        return 'request_id'\n",
    "    return  column_name\n",
    "\n",
    "def extract_table_data(sql_file, output_folder):\n",
    "    with open(sql_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    pattern = r'COPY\\s+public\\.\"(?P<table_name>\\w+)\"\\s+\\((?P<columns>[^\\)]+)\\)\\s+FROM\\s+stdin;\\n(?P<data>.*?)\\\\\\.\\n'\n",
    "    matches = re.finditer(pattern, content, re.DOTALL)\n",
    "\n",
    "    for match in matches:\n",
    "        table_name = match.group('table_name')\n",
    "        columns = match.group('columns').split(', ')\n",
    "        data = match.group('data').strip()\n",
    "        columns = [to_snake_case(column) for column in columns]\n",
    "        rows = [row.split('\\t') for row in data.splitlines()]\n",
    "        csv_file = os.path.join(output_folder, f'{table_name}.csv')\n",
    "\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(columns)\n",
    "            writer.writerows(rows)\n",
    "\n",
    "        print(f'Data from table \"{table_name}\" has been written to \"{csv_file}\".')\n",
    "test_params = [\n",
    "    \n",
    "    \"10_A+NA+AZA_30s_600ms_3\", \"130_A+NA+AZA_30s_600ms_3\",\"60_A+NA+AZA_30s_200ms_3\", \"60_A+NA+AZA_30s_1100ms_3\", \n",
    "    \"60_A+NA+AZA_5s_600ms_3\", \"60_A+NA+AZA_50s_600ms_3\", \"60_A+NA+AZA_30s_600ms_2\", \"60_A+NA+AZA_30s_600ms_8\"\n",
    "]\n",
    "base_output_folder = 'testData'\n",
    "if not os.path.exists(base_output_folder):\n",
    "    os.makedirs(base_output_folder)\n",
    "i = 20\n",
    "for param in test_params:\n",
    "    sql_file = f'logdatabase_test{i+1}.sql'\n",
    "    numbered_folder_name = f\"{i+1}.{param}\"\n",
    "    output_folder = os.path.join(base_output_folder, numbered_folder_name)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    print(f'Processing \"{sql_file}\" into folder \"{output_folder}\"...')\n",
    "    extract_table_data(os.path.join('testDataSql/', sql_file), output_folder)\n",
    "    i += 1\n",
    "print(\"All SQL files have been processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort data by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort():\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    for test_dir in file_paths:\n",
    "        test_dir_path = os.path.join(current_dir, test_dir)\n",
    "        \n",
    "        for root, _, files in os.walk(test_dir_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    df = df.sort_values(by='timestamp')\n",
    "                    df.to_csv(file_path, index=False)\n",
    "                    print(f'Sorted and saved: {file_path}')\n",
    "\n",
    "sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get complete market logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_market_logs():\n",
    "    current_dir = os.getcwd()\n",
    "    for test_dir in file_paths:\n",
    "        test_dir_path = os.path.join(current_dir, test_dir)\n",
    "        for root, _, files in os.walk(test_dir_path):\n",
    "            if 'stockApp_trafficlog.csv' in files and 'stockApp_marketlog.csv' in files:\n",
    "                traffic_log_path = os.path.join(root, test_dir_path,'stockApp_trafficlog.csv')\n",
    "                market_log_path = os.path.join(root, test_dir_path,'stockApp_marketlog.csv')\n",
    "                traffic_log_df = pd.read_csv(traffic_log_path)\n",
    "                market_log_df = pd.read_csv(market_log_path)\n",
    "                if 'user_class' in traffic_log_df.columns:\n",
    "                    complete_log_df = pd.merge(market_log_df, traffic_log_df[['request_id', 'api_time','user_class']], on='request_id', how='inner')\n",
    "                else:\n",
    "                    complete_log_df = pd.merge(market_log_df, traffic_log_df[['request_id', 'api_time']], on='request_id', how='inner')\n",
    "                if 'user_class' in complete_log_df.columns:\n",
    "                    complete_log_df = complete_log_df[['id', 'timestamp', 'api_method', 'application_time', 'database_time', 'endpoint_url', 'api_time','user_class']]\n",
    "                else:\n",
    "                    complete_log_df = complete_log_df[['id', 'timestamp', 'api_method', 'application_time', 'database_time', 'endpoint_url', 'api_time']]\n",
    "                complete_log_df = complete_log_df[~complete_log_df['endpoint_url'].isin(['/api/signIn', '/api/signUp', '/api/addCompany'])]\n",
    "                complete_log_df['timestamp'] = pd.to_datetime(complete_log_df['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "                complete_log_df = complete_log_df.dropna(subset=['timestamp'])\n",
    "                output_path = os.path.join(root, 'stockApp_complete_marketlog.csv')\n",
    "                complete_log_df.to_csv(output_path, index=False)\n",
    "                print(f\"File saved in: {output_path}\")\n",
    "\n",
    "get_complete_market_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get transactions times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_timestamp(df, target_timestamp):\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize(None)\n",
    "    if isinstance(target_timestamp, pd.Timestamp):\n",
    "        target_timestamp = target_timestamp.tz_localize(None) if target_timestamp.tzinfo else target_timestamp\n",
    "    return df.iloc[(df['timestamp'] - target_timestamp).abs().argsort()[:1]]['timestamp'].values[0]\n",
    "\n",
    "def extract_companies_group_trade():\n",
    "    current_dir = os.getcwd()\n",
    "    for test_dir in file_paths:\n",
    "        test_dir_path = os.path.join(current_dir, test_dir)\n",
    "        for root, _, files in os.walk(test_dir_path):\n",
    "            for file in files:\n",
    "                if not file.endswith('.csv'):\n",
    "                    continue\n",
    "                if file != 'stockApp_tradelog.csv':\n",
    "                    continue\n",
    "                df = pd.read_csv(os.path.join(root, test_dir_path, 'stockApp_tradelog.csv'))\n",
    "                df['company_ids'] = df['company_ids'].apply(lambda x: str(sorted(eval(x))))\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "                df = df[df['timestamp'] <= df['timestamp'].min() + timedelta(hours=1) + timedelta(seconds=4)]\n",
    "                unique_company_ids = df['company_ids'].unique()\n",
    "                dfs = []\n",
    "                for company_id_str in unique_company_ids:\n",
    "                    company_df = df[df['company_ids'] == company_id_str].drop(columns=['company_ids']).reset_index(drop=True)\n",
    "                    company_df['timestamp'] = pd.to_datetime(company_df['timestamp'], errors='coerce')\n",
    "                    dfs.append(company_df)\n",
    "                common_timestamps = sorted(set(dfs[0]['timestamp'].to_list()))\n",
    "                for df_group in dfs[1:]:\n",
    "                    for ts in common_timestamps:\n",
    "                        if ts not in df_group['timestamp'].values:\n",
    "                            closest_ts = find_closest_timestamp(df_group, ts)\n",
    "                            common_timestamps.remove(ts)\n",
    "                            common_timestamps.append(closest_ts)\n",
    "                common_timestamps = [\n",
    "                    pd.to_datetime(ts).tz_localize(None) if isinstance(ts, pd.Timestamp) else pd.to_datetime(ts)\n",
    "                    for ts in common_timestamps\n",
    "                ]\n",
    "                result_df = pd.DataFrame([{\n",
    "                        'timestamp': min(common_timestamps) - timedelta(minutes=2),\n",
    "                        'application_time_trade': 0,\n",
    "                        'database_time_trade': 0,\n",
    "                        'number_of_sell_offers': 0,\n",
    "                        'number_of_buy_offers': 0\n",
    "                    }])\n",
    "                for ts in common_timestamps:\n",
    "                    pre_row = {\n",
    "                        'timestamp': ts - timedelta(seconds=0.1),\n",
    "                        'application_time_trade': 0,\n",
    "                        'database_time_trade': 0,\n",
    "                        'number_of_sell_offers': 0,\n",
    "                        'number_of_buy_offers': 0\n",
    "                    }\n",
    "                    current_row = {\n",
    "                        'timestamp': ts,\n",
    "                        'application_time_trade': sum([df_group[df_group['timestamp'] == find_closest_timestamp(df_group, ts)]['application_time'].values[0] for df_group in dfs]) / len(dfs),\n",
    "                        'database_time_trade': sum([df_group[df_group['timestamp'] == find_closest_timestamp(df_group, ts)]['database_time'].values[0] for df_group in dfs]) / len(dfs),\n",
    "                        'number_of_sell_offers': sum([df_group[df_group['timestamp'] == find_closest_timestamp(df_group, ts)]['number_of_sell_offers'].values[0] for df_group in dfs]),\n",
    "                        'number_of_buy_offers': sum([df_group[df_group['timestamp'] == find_closest_timestamp(df_group, ts)]['number_of_buy_offers'].values[0] for df_group in dfs])\n",
    "                    }\n",
    "                    max_application_time = max(df_group[df_group['timestamp'] == find_closest_timestamp(df_group, ts)]['application_time'].values[0] for df_group in dfs)\n",
    "                    post_row = {\n",
    "                        'timestamp': ts + timedelta(seconds=0.1 + 1.5 * max_application_time),\n",
    "                        'application_time_trade': 0,\n",
    "                        'database_time_trade': 0,\n",
    "                        'number_of_sell_offers': 0,\n",
    "                        'number_of_buy_offers': 0\n",
    "                    }\n",
    "                    result_df = pd.concat([result_df, pd.DataFrame([pre_row]), pd.DataFrame([current_row]), pd.DataFrame([post_row])], ignore_index=True)\n",
    "                output_path = os.path.join(root, \"stockApp_tradelog_mean.csv\")\n",
    "                result_df.to_csv(output_path, index=False)\n",
    "                print(f\"File saved in: {output_path}\")\n",
    "\n",
    "extract_companies_group_trade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate containers into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_containers_cpu_usage():\n",
    "    current_dir = os.getcwd()\n",
    "    for test_dir in file_paths:\n",
    "        test_dir_path = os.path.join(current_dir, test_dir)\n",
    "        for root, _, files in os.walk(test_dir_path):\n",
    "            for file in files:\n",
    "                if not file.endswith('.csv'):\n",
    "                    continue\n",
    "                if file != 'stockApp_cpu.csv':\n",
    "                    continue\n",
    "                df = pd.read_csv(os.path.join(root, test_dir_path,'stockApp_cpu.csv'))\n",
    "                contener_ids = ['stockproject-web-1',\n",
    "                                'stockproject-db-1', \n",
    "                                'stockproject-db_test-1',\n",
    "                                'stockproject-locust-1',\n",
    "                                'stockproject-celery_worker_execute_transactions-1']\n",
    "                contener_names =['web','db','db_test','test','transaction']\n",
    "                for i,contener_id_str in  enumerate(contener_ids,start=0):\n",
    "                    contener_df = df[df['contener_id'] == contener_id_str].drop(columns=['memory_usage','contener_id'])\n",
    "                    contener_df['timestamp'] = pd.to_datetime(contener_df['timestamp'], errors='coerce').dt.tz_localize(None)\n",
    "                    contener_df['timestamp'] = contener_df['timestamp'] - timedelta(hours=2)\n",
    "                    contener_df.rename(columns={'cpu_usage': f'cpu_usage_{contener_names[i]}'},inplace=True)\n",
    "                    file_name = f\"stockApp_cpu_{contener_names[i]}.csv\"\n",
    "                    output_path = os.path.join(root,file_name)\n",
    "                    contener_df.to_csv(output_path, index=False)\n",
    "                    print(f\"File saved in: {output_path}\")\n",
    "extract_containers_cpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeData():\n",
    "    for test_dir in file_paths:\n",
    "        dfs = []\n",
    "        dfs.append(pd.read_csv(f'{test_dir}/stockApp_complete_marketlog.csv'))\n",
    "        dfs.append(pd.read_csv(f'{test_dir}/stockApp_tradelog_mean.csv'))\n",
    "        dfs.append(pd.read_csv(f'{test_dir}/stockApp_cpu_db_test.csv'))\n",
    "        dfs.append(pd.read_csv(f'{test_dir}/stockApp_cpu_db.csv'))\n",
    "        dfs.append(pd.read_csv(f'{test_dir}/stockApp_cpu_test.csv'))\n",
    "        dfs.append(pd.read_csv(f'{test_dir}/stockApp_cpu_transaction.csv'))\n",
    "        dfs.append(pd.read_csv(f'{test_dir}/stockApp_cpu_web.csv'))\n",
    "        for df in dfs:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "            nan_rows = df[df['timestamp'].isna()]\n",
    "            if(len(nan_rows.index) >=1):\n",
    "                print(nan_rows)\n",
    "                df = df.drop(df[df['timestamp'].isna()])\n",
    "            df = df.drop(columns=[col for col in df.columns if col.startswith('id')], errors='ignore')\n",
    "        merged_data = pd.merge_asof(dfs[0], \n",
    "                              dfs[1].sort_values('timestamp'), \n",
    "                              on='timestamp', \n",
    "                              direction='nearest')\n",
    "        for i in range(2,len(dfs),1):\n",
    "            merged_data = pd.merge_asof(merged_data, \n",
    "                              dfs[i], \n",
    "                              on='timestamp', \n",
    "                              direction='nearest')\n",
    "            merged_data = merged_data.drop(columns=[col for col in merged_data.columns if col.startswith('id')], errors='ignore')\n",
    "        \n",
    "        merged_data.to_csv(f'{test_dir}/stockApp_merged_data.csv', index=False)\n",
    "        if os.path.exists(f'{test_dir}/merged_data.csv'):\n",
    "            os.remove(f'{test_dir}/merged_data.csv')\n",
    "        print(f\"File saved in: {test_dir}stockApp_merged_data.csv'\")\n",
    "\n",
    "mergeData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEST_DATA_DIR = pathlib.Path('testData')\n",
    "\n",
    "test_dirs = sorted([p for p in TEST_DATA_DIR.iterdir() if p.is_dir()])\n",
    "\n",
    "print(f' {len(test_dirs)} folders')\n",
    "\n",
    "for test_dir in test_dirs:\n",
    "    label = test_dir.name\n",
    "    merged_file = test_dir / 'stockApp_merged_data.csv'\n",
    "    traffic_file = test_dir / 'stockApp_trafficlog.csv' \n",
    "\n",
    "    if merged_file.exists():\n",
    "        df = pd.read_csv(merged_file, parse_dates=['timestamp'])\n",
    "    elif traffic_file.exists():\n",
    "        df = pd.read_csv(traffic_file, parse_dates=['timestamp'])\n",
    "    else:\n",
    "        print(f'Lack merged/traffic file in {label} ')\n",
    "        continue\n",
    "\n",
    "    total = len(df)\n",
    "    by_class = df['user_class'].value_counts(dropna=False)\n",
    "    by_endpoint = df['endpoint_url'].value_counts(dropna=False)\n",
    "\n",
    "    # Requests‑per‑second\n",
    "    df['timestamp_sec'] = df['timestamp'].dt.floor('s')\n",
    "    rps = df.groupby('timestamp_sec').size()\n",
    "\n",
    "    print(f'\\n=== Test {label} ===')\n",
    "    print(f'Requests: {total}')\n",
    "\n",
    "    # 1. RPS\n",
    "    plt.figure()\n",
    "    rps.plot()\n",
    "    plt.title(f'RPS – {label}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Requests / s')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Class\n",
    "    plt.figure()\n",
    "    by_class.plot(kind='bar')\n",
    "    plt.title(f'user_class – {label}')\n",
    "    plt.ylabel('Requests')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Endpoint\n",
    "    plt.figure()\n",
    "    by_endpoint.head(20).plot(kind='bar')\n",
    "    plt.title(f'Endpoints – {label}')\n",
    "    plt.ylabel('Requests')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
