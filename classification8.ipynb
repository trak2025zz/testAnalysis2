{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c85b876",
   "metadata": {},
   "source": [
    "# ML regresja i klasyfikacja na Apple Silicon (M1)\n",
    "\n",
    "Ten notatnik odtwarza pipeline z Twojego projektu, ale działa **natywnie** na macOS arm64. Zawiera zarówno regresję na XGBoost, jak i klasyfikację wieloklasową (RandomForest, ExtraTrees, DecisionTree, KNN, LinearSVC, MLP, XGB).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ccf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "import itertools, time, sys, warnings, pathlib\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    _xgb_ok = True\n",
    "except (ImportError, RuntimeError) as err:\n",
    "    _xgb_ok = False\n",
    "    warnings.warn(\n",
    "        f'Nie udało się załadować xgboost ({err}). '\n",
    "        'Regresja i klasyfikacja XGB zostaną pominięte.', RuntimeWarning\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de422488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ścieżki do zbiorów danych (z poprzedniego notatnika) ---\n",
    "file_paths = [\n",
    "    #'testData/1.10_A_30s_600ms_3/',\n",
    "    #'testData/2.50_A_30s_600ms_3/',\n",
    "    #'testData/3.90_A_30s_600ms_3/',\n",
    "    'testData/4.130_A_30s_600ms_3/',\n",
    "    #'testData/5.60_A_30s_200ms_3/',\n",
    "    #'testData/6.60_A_30s_500ms_3/',\n",
    "    #'testData/7.60_A_30s_800ms_3/',\n",
    "    #'testData/8.60_A_30s_1100ms_3/',\n",
    "    #'testData/9.60_A_5s_600ms_3/',\n",
    "    #'testData/10.60_A_20s_600ms_3/',\n",
    "    #'testData/11.60_A_35s_600ms_3/',\n",
    "    #'testData/12.60_A_50s_600ms_3/',\n",
    "    'testData/13.60_A_30s_600ms_2/',\n",
    "    #'testData/14.60_A_30s_600ms_4/',\n",
    "    #'testData/15.60_A_30s_600ms_6/',\n",
    "    #'testData/16.60_A_30s_600ms_8/',\n",
    "    #'testData/17.60_A_30s_600ms_3/',\n",
    "    'testData/18.60_A+NA+AZA_30s_600ms_3/',\n",
    "    #'testData/19.60_A+NA_30s_600ms_3/',\n",
    "    #'testData/20.60_A+AZA_30s_600ms_3/',\n",
    "]\n",
    "file_paths2 = [\n",
    "    'testData/21.10_A+NA+AZA_30s_600ms_3/',\n",
    "    'testData/22.130_A+NA+AZA_30s_600ms_3/',\n",
    "    #'testData/23.60_A+NA+AZA_30s_200ms_3/',\n",
    "    #'testData/24.60_A+NA+AZA_30s_1100ms_3/',\n",
    "    #'testData/25.60_A+NA+AZA_5s_600ms_3/',\n",
    "    #'testData/26.60_A+NA+AZA_50s_600ms_3/',\n",
    "    #'testData/27.60_A+NA+AZA_30s_600ms_2/',\n",
    "    #'testData/28.60_A+NA+AZA_30s_600ms_8/',\n",
    "]\n",
    "\n",
    "labels = [ _[12:-1] if _ [11] == '.' else _[11:-1] for _ in file_paths ]\n",
    "labels2 = [ _[12:-1] if _ [11] == '.' else _[11:-1] for _ in file_paths2 ]\n",
    "# Aliasy dla kompatybilności ze starym notatnikiem\n",
    "xgbTree = globals().get('xgb_tree')\n",
    "multiClass = globals().get('multi_class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94975c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Regresja -----------------\n",
    "def xgb_tree(csv_path: pathlib.Path, target: str) -> None:\n",
    "    if not _xgb_ok:\n",
    "        print('⚠️  xgboost niedostępny – pomijam regresję')\n",
    "        return\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'user_class' in df.columns:\n",
    "        df = df.drop(columns='user_class')\n",
    "    df = df.drop(columns=[\n",
    "        'timestamp', 'endpoint_url', 'api_method',\n",
    "        'application_time_trade', 'database_time_trade',\n",
    "        'number_of_sell_offers', 'number_of_buy_offers'\n",
    "    ], errors='ignore')\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    dtrain, dtest = xgb.DMatrix(X_tr, label=y_tr), xgb.DMatrix(X_te, label=y_te)\n",
    "    params = dict(objective='reg:squarederror', tree_method='hist',\n",
    "                  device='cpu', max_depth=10, learning_rate=0.1)\n",
    "    print(f'Trenuję XGB regresor dla “{target}” w {csv_path.parent.name}')\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "    preds = model.predict(dtest)\n",
    "    print(f'MSE={mean_squared_error(y_te, preds):.4f} | R²={r2_score(y_te, preds):.4f}')\n",
    "# --------------- Klasyfikacja --------------\n",
    "_CLASSIFIERS = {\n",
    "    'RandomForest': RandomForestClassifier(max_depth=8, random_state=42,\n",
    "                                           criterion='gini', n_estimators=500,\n",
    "                                           min_samples_split=8, max_features='sqrt', bootstrap=False),\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=1000, random_state=42,\n",
    "                                       criterion='gini', max_depth=10, min_samples_split=4,\n",
    "                                       max_features='sqrt', bootstrap=False),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42, max_depth=8,\n",
    "                                           criterion='gini', max_features='sqrt',\n",
    "                                           min_samples_split=8, splitter='best'),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7, algorithm='auto', leaf_size=20),\n",
    "    'LinearSVC': LinearSVC(multi_class='ovr', class_weight='balanced',\n",
    "                           max_iter=10000, C=10, loss='squared_hinge',\n",
    "                           penalty='l1', dual=False, random_state=42),\n",
    "    'MLP': MLPClassifier(random_state=42, alpha=1e-4, learning_rate='constant',\n",
    "                         max_iter=200, batch_size=16, hidden_layer_sizes=(120, 80, 40),\n",
    "                         solver='adam', activation='relu'),\n",
    "}\n",
    "if _xgb_ok:\n",
    "    _CLASSIFIERS['XGB'] = xgb.XGBClassifier(booster='gbtree', objective='multi:softmax',\n",
    "                                            eval_metric='mlogloss', random_state=42)\n",
    "def _scale_if_needed(name, X_tr, X_te):\n",
    "    if name in ('LinearSVC', 'MLP'):\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        return scaler.transform(X_tr), scaler.transform(X_te)\n",
    "    return X_tr, X_te\n",
    "def multi_class(csv_path: pathlib.Path, label_col='user_class'):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    classes = sorted(df[label_col].unique())\n",
    "    df['mapped'] = df[label_col].map({c:i for i,c in enumerate(classes)})\n",
    "    if label_col != 'endpoint_url':\n",
    "        df['endpoint_url_mapped'] = df['endpoint_url'].map(\n",
    "            {u:i for i,u in enumerate(sorted(df['endpoint_url'].unique()))})\n",
    "        features = ['endpoint_url_mapped','application_time','database_time','api_time',\n",
    "                    'cpu_usage_db_test','cpu_usage_db','cpu_usage_web']\n",
    "    else:\n",
    "        features = ['application_time','database_time','api_time',\n",
    "                    'cpu_usage_db_test','cpu_usage_db','cpu_usage_web']\n",
    "    X, y = df[features], df['mapped']\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, train_size=0.8,\n",
    "                                              random_state=42, stratify=y)\n",
    "    for name, clf in _CLASSIFIERS.items():\n",
    "        Xt, Xe = _scale_if_needed(name, X_tr, X_te)\n",
    "        start = time.time()\n",
    "        clf.fit(Xt, y_tr)\n",
    "        preds = clf.predict(Xe)\n",
    "        print(f'\\n{name} | czas {time.time()-start:.2f}s')\n",
    "        print(classification_report(y_te, preds, target_names=classes, zero_division=0))\n",
    "        cm = confusion_matrix(y_te, preds)\n",
    "        _plot_cm(cm, classes, title=f'{csv_path.parent.name} | {name}')\n",
    "def _plot_cm(cm, labels, title='CM'):\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set_xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(len(labels)), labels)\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('True'); ax.set_title(title)\n",
    "    thresh = cm.max()/2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i,j], ha='center', va='center',\n",
    "                    color='white' if cm[i,j]>thresh else 'black')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cebea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = pathlib.Path('testData')  # <= ZMIEŃ NA SWOJĄ ŚCIEŻKĘ\n",
    "assert DATA_ROOT.exists(), f'Ścieżka {DATA_ROOT} nie istnieje'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Automatyczne tworzenie list plików CSV na podstawie odkomentowanych katalogów ---\n",
    "# Jeśli w file_paths (1‑20) lub file_paths2 (21‑28) wszystko jest zakomentowane,\n",
    "# notebook skanuje całe DATA_ROOT tak jak dotychczas.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _csvs_from_dirs(dir_list, fallback_root):\n",
    "    dirs = [d for d in dir_list if isinstance(d,str) and d.strip() and not d.strip().startswith('#')]\n",
    "    if dirs:\n",
    "        return [Path(d) / 'stockApp_merged_data.csv' for d in dirs]\n",
    "    else:\n",
    "        return list(fallback_root.rglob('*_merged_data.csv'))\n",
    "\n",
    "CSV_FILES_REG = _csvs_from_dirs(file_paths, DATA_ROOT)   # regresja\n",
    "CSV_FILES_CLS = _csvs_from_dirs(file_paths2, DATA_ROOT)  # klasyfikacja\n",
    "print(f'📈 CSV do regresji: {len(CSV_FILES_REG)} plików')\n",
    "print(f'🔤 CSV do klasyfikacji: {len(CSV_FILES_CLS)} plików')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e11199",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _xgb_ok:\n",
    "    targets = ['api_time', 'application_time', 'database_time']\n",
    "    for csv_file in CSV_FILES_REG:\n",
    "        for t in targets:\n",
    "            xgb_tree(csv_file, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6749ed96",
   "metadata": {},
   "source": [
    "## 4 – Klasyfikacja wieloklasowa\n",
    "Wykonaj komórkę poniżej – otrzymasz metryki i macierze konfuzji dla kilku klasyfikatorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08359f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_file in CSV_FILES_CLS:\n",
    "    multi_class(csv_file, label_col='user_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb54993",
   "metadata": {},
   "source": [
    "### To wszystko!\n",
    "Jeśli chcesz zmienić hiperparametry, edytuj odpowiednie sekcje z definicjami modeli."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
